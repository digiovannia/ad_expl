#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#%%
import numpy as np
import itertools
from math import floor, ceil
import nashpy as nash
import os
import sys


GAM = 0.9
LEARN = 5
TAU = 100
VERBOSE_ALGOS = False 
VERBOSE_TRIAL = False 
VERBOSE_EXPERIMENT = False 
SPAN_EST = 0.001
NUM_TRIALS = 50
FOLLOW_CONST = 0.005
TUNING_FACTOR = 0.005
LAG_CONST = 0.05


np.set_printoptions(edgeitems=30, linewidth=100000, 
    formatter=dict(float=lambda x: "%.3g" % x))
np.seterr(divide='ignore')


TIME = int(1e5)*2
STOPPING = False
TAU1 = TIME // 20
MANI_EPOCH = 1
MANI_H = max(5, TIME // 20)
TAU = max(5, TIME // 1000)
TAU2 = 6*TAU1


def adv_convert(game_1, game_2):
    '''
    Computes advantage games from base games
    '''
    vsec1, vsec2 = security(game_1, game_2)[-2:]
    ag_1 = game_1 - vsec1
    ag_2 = game_2 - vsec2
    return ag_1, ag_2

    
def pair_barg(game_1, game_2, a1, a2, b1, b2, eps=0, p_ix=0, K=2, soln='ebs'):
    '''
    For a fixed pair of outcomes (a1, a2) and (b1, b2), computes the
    candidate EBS/bully value and weight p.
    '''
    ag_1, ag_2 = adv_convert(game_1, game_2)
    ra1 = ag_1[a1,a2]
    ra2 = ag_2[a1,a2]
    rb1 = ag_1[b1,b2]
    rb2 = ag_2[b1,b2]
    excla2 = np.ma.array(ag_2[a1], mask=False)
    exclb2 = np.ma.array(ag_2[b1], mask=False)
    excla1 = np.ma.array(ag_1[:,a2], mask=False)
    exclb1 = np.ma.array(ag_1[:,b2], mask=False)
    excla2.mask[a2] = True
    exclb2.mask[b2] = True
    excla1.mask[a1] = True
    exclb1.mask[b1] = True
    ca2 = np.max(excla2) - ra2
    cb2 = np.max(exclb2) - rb2
    ca1 = np.max(excla1) - ra1
    cb1 = np.max(exclb1) - rb1
    # Note that the threshold is either an upper or lower bound
    # depending on sign of ra1 - rb2, or ra2 - rb2
    threshold_1 = (np.max([ca1, cb1]) + eps - K*rb1)/(K*(ra1 - rb1))
    threshold_2 = (np.max([ca2, cb2]) + eps - K*rb2)/(K*(ra2 - rb2))
    # Checking for enforceability
    if soln == 'ebs':
        if ra1 <= rb1 and ra2 <= rb2:
            p = 0
        elif ra1 >= rb1 and ra2 >= rb2:
            p = 1
        else:
            a = (rb2 - rb1)/(ra1 - rb1 + rb2 - ra2)
            if ra1 > rb1:
                if ra2 > rb2:
                    p = max(a, max(threshold_1, threshold_2))
                else:
                    if threshold_1 > threshold_2:
                        p = -1
                    else:
                        p = min(max(a, threshold_1), threshold_2)
            else:
                if ra2 < rb2:
                    p = min(a, min(threshold_1, threshold_2))
                else:
                    if threshold_1 < threshold_2:
                        p = -1
                    else:
                        p = max(min(a, threshold_1), threshold_2)
            # Might end up being outside [0,1] in which case not enforceable
    if soln == 'bully':
        if ra2 <= rb2: # in which case we want to minimize p if p_ix
            if ra1 <= rb1: # want to minimize p if not p_ix
                p = 0 if threshold_1 >= 0 and threshold_2 >= 0 else -1
            else: # maximize p if not p_ix
                if threshold_1 > threshold_2:
                    p = -1
                else:
                    p = max(0, threshold_1) if p_ix else min(1, threshold_2)
        else: # maximize p if p_ix
            if ra1 >= rb1: # maximize p if not p_ix
                p = 1 if threshold_1 <= 1 and threshold_2 <= 1 else -1
            else:
                if threshold_1 < threshold_2:
                    p = -1
                else:
                    p = min(1, threshold_1) if p_ix else max(0, threshold_2)
    if np.abs(threshold_1) == np.inf:
        p = 0 if np.max([ca1, cb1]) + eps <= K*ra1 else -1
    if np.abs(threshold_2) == np.inf:
        p = 0 if np.max([ca2, cb2]) + eps <= K*ra2 else -1
    ebs_1 = ra1*p + rb1*(1-p)
    ebs_2 = ra2*p + rb2*(1-p)
    if soln == 'ebs':
        return p, (min(ebs_1, ebs_2) if ebs_1 >= 0 and ebs_2 >= 0 else -np.inf)
    if soln == 'bully':
        return p, ebs_2 if p_ix else ebs_1


def bargain(game_1, game_2, solution='ebs', eps=0.001,
            p_ix=0, K=1):
    '''
    Given two game matrices (not yet converted to advantages), computes EBS or Bully
    '''
    best_val = -np.inf
    best_a1 = 0
    best_a2 = 0
    best_b1 = 0
    best_b2 = 0
    best_p = 0
    ag = adv_convert(game_1, game_2)
    for a1 in range(game_1.shape[0]):
        for a2 in range(game_1.shape[1]):
            for b1 in range(game_1.shape[0]):
                for b2 in range(game_1.shape[1]):
                    p, val = pair_barg(game_1, game_2, a1, a2, b1, b2, eps=eps,
                                      p_ix=p_ix, K=K, soln=solution)
                    if val > best_val and (0 <= p <= 1):
                        if (ag[1-p_ix][a1,a2]*p + ag[1-p_ix][b1,b2]*(1-p) >= 0 and
                          ag[p_ix][a1,a2]*p + ag[p_ix][b1,b2]*(1-p) >= 0):
                            best_val = val
                            best_a1 = a1
                            best_a2 = a2
                            best_b1 = b1
                            best_b2 = b2
                            best_p = p
    ra1 = game_1[best_a1,best_a2]
    ra2 = game_2[best_a1,best_a2]
    rb1 = game_1[best_b1,best_b2]
    rb2 = game_2[best_b1,best_b2]
    ebs_1 = ra1*best_p + rb1*(1-best_p)
    ebs_2 = ra2*best_p + rb2*(1-best_p)
    if (ag[0][best_a1,best_a2]*best_p + ag[0][best_b1,best_b2]*(1-best_p) >= 0 and
          ag[1][best_a1,best_a2]*best_p + ag[1][best_b1,best_b2]*(1-best_p) >= 0):
        return (best_a1, best_a2), (best_b1, best_b2), best_p, (ebs_1, ebs_2)
    return None


def punish(game_1, game_2, p_ix=0):
    '''
    Computes minmax vector
    '''
    mvec12, mvec21 = security(game_1, game_2, mini=True)[1:3]
    if p_ix == 0:
        return mvec21
    return mvec12
    
    
def defection_test(s, K, geom_states, targets, Kp, p_ix=0):
    '''
    Checks if a state indicates that other player defected
    '''
    broke = False
    both_broke = False
    try:
        sub = max(0, ceil(Kp[1-p_ix]))
    except:
        sub = max(0, Kp[1-p_ix])
    for k in range(K - sub, K):
        other_tar = targets[geom_states[s][(2*K+(K+1)*p_ix)+k]][1-p_ix]
        self_tar = targets[geom_states[s][(2*K+(K+1)*(1-p_ix))+k]][p_ix]
        broke = broke or geom_states[s][(1-p_ix)*K+k] != other_tar
        both_broke = both_broke or broke or geom_states[s][(p_ix)*K+k] != self_tar
    return broke, both_broke
    

def godfather(rg, p_ix=0, solution='bully', eps=0.1, generosity=0):
    '''
    Godfather policy and value with respect to EBS or Bully
    '''
    try:
        target_1, target_2, w, vebs = bargain(rg.rewards_1[0],
          rg.rewards_2[0], solution=solution, eps=eps, p_ix=p_ix, K=rg.K)
    except:
        return rg.safe[p_ix], rg.vsec[p_ix], 0, 1
    targets = [target_1, target_2]
    vbul = vebs[p_ix]
    pun = rg.attack[p_ix]
    pol1 = np.zeros((rg.S, rg.A))
    ag_1, ag_2 = adv_convert(rg.rewards_1[0], rg.rewards_2[0])
    a1, a2 = target_1
    b1, b2 = target_2
    ra1 = ag_1[a1,a2]
    ra2 = ag_2[a1,a2]
    rb1 = ag_1[b1,b2]
    rb2 = ag_2[b1,b2]
    excla2 = np.ma.array(ag_2[a1], mask=False)
    exclb2 = np.ma.array(ag_2[b1], mask=False)
    excla1 = np.ma.array(ag_1[:,a2], mask=False)
    exclb1 = np.ma.array(ag_1[:,b2], mask=False)
    excla2.mask[a2] = True
    exclb2.mask[b2] = True
    excla1.mask[a1] = True
    exclb1.mask[b1] = True
    ca2 = np.max(excla2) - ra2
    cb2 = np.max(exclb2) - rb2
    ca1 = np.max(excla1) - ra1
    cb1 = np.max(exclb1) - rb1
    set1 = [ca1, cb1] if w not in [0, 1] else ([cb1] if w == 0 else [ca1])
    set2 = [ca2, cb2] if w not in [0, 1] else ([cb2] if w == 0 else [ca2])
    vsec1, vsec2 = security(rg.rewards_1[0], rg.rewards_2[0])[4:]
    Kp = {0: round((np.max(set1) + eps)/(vebs[0] - vsec1), 8),
          1: round((np.max(set2) + eps)/(vebs[1] - vsec2), 8)}

    for s in range(rg.S):
        broke, both_broke = defection_test(s, rg.K, rg.geom_states, targets,
          Kp, p_ix=p_ix)
        coop = np.zeros(rg.A)
        coop[targets[rg.geom_states[s][-1-(rg.K+1)*(1-p_ix)]][p_ix]] = 1
        if broke:
            pol1[s] = generosity*coop + (1-generosity)*pun
        else:
            pol1[s,targets[rg.geom_states[s][-1-(rg.K+1)*(1-p_ix)]][p_ix]] = 1
    return distribut(pol1), vbul, w


def security(game_1, game_2, mini=False):
    '''
    Maximin and minimax vectors and values
    '''
    vee1 = np.inf if mini else -np.inf
    vee2 = np.inf if mini else -np.inf
    g1 = nash.Game(game_1)
    nes1 = list(g1.support_enumeration())
    if not nes1: # Sometimes the first method of NE computation fails
        nes1 = list(g1.vertex_enumeration())
    for ne in nes1:
        mv1, mv2 = ne
        value = mv1.dot(game_1).dot(mv2)
        if (mini and value < vee1) or (not mini and value > vee1):
            vee1 = value
            mvec11, mvec12 = mv1, mv2
    vsec1 = vee1
    g2 = nash.Game(-game_2)
    nes2 = list(g2.support_enumeration())
    if not nes2:
        nes2 = list(g2.vertex_enumeration())
    for ne in nes2:
        mv1, mv2 = ne
        value = mv1.dot(game_2).dot(mv2)
        if (mini and value < vee2) or (not mini and value > vee2):
            vee2 = value
            mvec21, mvec22 = mv1, mv2
    vsec2 = vee2
    return mvec11, mvec12, mvec21, mvec22, vsec1, vsec2


def maximin(game_1, game_2, S, A, p_ix=0):
    '''
    Makes a (stationary) policy based on maximin vector
    '''
    mvec11, mvec12, mvec21, mvec22, vsec1, vsec2 = security(game_1, game_2)
    pol1 = np.zeros((S, A))
    if p_ix == 0:
        pol1[:] = mvec11
        return pol1, vsec1
    pol1[:] = mvec22
    return pol1, vsec2


def value_iter(S, A, rewards_1, ptP, pol2, gam, tol, polbuild=True):
    '''
    Action-value iteration for Q* and optimal policy (using known ptPs)
    '''
    Q = np.random.rand(S, A)
    delta = np.inf
    # expected reward of action in current state, where expectation is over
    # other player's policy
    expect_rewards = np.einsum('ijk,ik->ij', rewards_1, pol2)
    while delta > tol:
        delta = 0
        for s in range(S):
            a = np.random.choice(np.arange(A))
            qval = Q[s,a]
            Q[s,a] = expect_rewards[s,a] + gam*(ptP[s,a].dot(Q.max(axis=1)))
            delta = np.max([delta, abs(qval - Q[s,a])])
    if polbuild:
        policy = np.zeros((S, A))
        for s in range(S):
            policy[s,np.argmax(Q[s])] = 1
        return policy, Q
    return Q


def bestresp(pol2, tP, S, A, rewards_1, rewards_2,
             gam=0.99, delt=1e-3, p_ix=0):
    if p_ix == 0:
        ptP = np.einsum('ijkl,ik->ijl', tP, pol2)
        Q = value_iter(S, A, rewards_1, ptP, pol2, gam, delt,
                   polbuild=False)
    else:
        ptP = np.einsum('ijkl,ij->ikl', tP, pol2)
        Q = value_iter(S, A, np.swapaxes(rewards_2, 1, 2), ptP, pol2, gam, delt,
                   polbuild=False)
    policy = np.zeros((S, A))
    policy[np.arange(S),np.argmax(Q, axis=1)] = 1
    return policy


def policynorm(M1, M2):
    '''
    Norm used for Manipulator's stationarity tests
    '''
    return np.max(np.abs(M1 - M2).sum(axis=1))


def oppstationary(game_states, actions, S, A, eps3, tau1):
    '''
    Does the opponent's action history imply a stationary policy?
    '''
    Nsa_pre = np.zeros((S, A))
    Nsa_post = np.zeros((S, A))
    for i, st in enumerate(game_states[:tau1]):
        Nsa_pre[st,actions[i]] += 1
    for i, st in enumerate(game_states[-tau1:]):
        Nsa_post[st,actions[i]] += 1
    M1 = Nsa_pre / Nsa_pre.sum(axis=1)[:,None]
    M2 = Nsa_post / Nsa_post.sum(axis=1)[:,None]
    M1 = np.nan_to_num(M1, nan=1/M1.shape[1])
    M2 = np.nan_to_num(M2, nan=1/M2.shape[1])
    return policynorm(M1, M2) < eps3


def mrp_value(rewards, pol1, pol2, tP):
    '''
    Computes the value of S++'s best response policy against the current
    model of the opponent
    '''
    markov = np.einsum('ijk,ij->ik', np.einsum('ijkl,ik->ijl', tP, pol2), pol1)
    evals, evecs = np.linalg.eig(markov.T)
    evec1 = evecs[:,np.isclose(evals, 1)]
    evec1 = evec1[:,0]
    stationary = evec1 / evec1.sum()
    stationary = stationary.real
    return stationary.dot(rewards.ravel())


def state_map(s, rg):
    '''
    Maps state to the simplified state used by S++
    '''
    a1 = rg.geom_states[s][rg.K-1]
    a2 = rg.geom_states[s][2*rg.K-1]
    return a1*rg.A+a2


def distribut(arr):
    '''
    Corrects weird numerical errors for prob distributions
    '''
    a = arr - np.minimum(0, np.min(arr, axis=1))[:,None]
    return a / np.sum(a, axis=1)[:,None]


def simple_tp(S, A, geom_states):
    '''
    Transition probs for 1-memory fictitious play
    '''
    tP = np.zeros((S, A, A, S))
    for s in range(S):
        for a1 in range(A):
            for a2 in range(A):
                for sp in range(S):
                    end = geom_states[sp]
                    if end[0] == a1 and end[1] == a2:
                        tP[s,a1,a2,sp] = 1
    return tP


def transition(S, A, geom_states, K, probs_1, probs_2):
    '''
    We suppose that each player publicly shares their randomization signal.
    
    Transition probabilities based on two possibly differing randomization
    devices. If players disagree, they're still responding to the same
    randomization device, so still correlated, just not perfectly.
    
    p(0,0) = min(probs_1[0], probs_2[0])
    p(1,0) = max(0, probs_2[0] - probs_1[0])
    p(0,1) = max(0, probs_1[0] - probs_2[0])
    p(1,1) = min(probs_1[1], probs_2[1])
    '''
    plist = [probs_1, probs_2]
    tP = np.zeros((S, A, A, S))
    for s in range(S):
        for a1 in range(A):
            for a2 in range(A):
                for sp in range(S):
                    start = geom_states[s]
                    end = geom_states[sp]
                    if K == 1:
                        if (end[0] == a1 and end[1] == a2 and
                          start[-1] == end[-2] and start[-(K+2)] == end[-(K+3)]):
                            if end[-1] == end[-(K+2)]:
                                tP[s,a1,a2,sp] = min(plist[0][end[-1]],
                                  plist[1][end[-1]])
                            else:
                                tP[s,a1,a2,sp] = max(0,
                                  plist[end[-(K+2)]][0] - plist[end[-1]][0])
                    else:
                        if (start[1:K] == end[:(K-1)] and
                          start[(K+1):(2*K)] == end[K:(2*K-1)] and
                          start[-K:] == end[-(K+1):-1] and
                          start[(1-2*(K+1)):(-(K+1))] == end[(-2*(K+1)):(-(K+2))]):
                            if end[K-1] == a1 and end[2*K-1] == a2:
                                if end[-1] == end[-(K+2)]:
                                    tP[s,a1,a2,sp] = min(plist[0][end[-1]],
                                      plist[1][end[-1]])
                                else:
                                    tP[s,a1,a2,sp] = max(0,
                                      plist[end[-(K+2)]][0] - plist[end[-1]][0])
    return tP


def mbrl(rg, Nsa, tP, p_ix=0, ess=False):
    '''
    Computes empirical model of opponent's policy and a best
    response (stateful fictitious play)
    '''
    oppol = Nsa / Nsa.sum(axis=1)[:,None]
    oppol = np.nan_to_num(oppol, nan=1/oppol.shape[1])
    if ess:
        return oppol, bestresp(oppol, tP, rg.sfp, rg.A, rg.s_rewards_1,
          rg.s_rewards_2, gam=GAM, delt=1e-3, p_ix=p_ix)
    return oppol, bestresp(oppol, tP, rg.S, rg.A, rg.rewards_1,
      rg.rewards_2, gam=GAM, delt=1e-3, p_ix=p_ix)


'''
Auxiliary functions for algorithms
'''


def construct_targets(base_1, base_2, tol=1e-10):
    '''
    Computes two sets of targets for S++:
    1) targets that are individually rational
    2) 1-step enforceable
    '''
    vsec1, vsec2 = security(base_1, base_2)[4:]
    ir_targets = set()
    enf_targets = set()
    for a1 in range(base_1.shape[0]):
        for b1 in range(base_1.shape[0]):
            for a2 in range(base_1.shape[1]):
                for b2 in range(base_1.shape[1]):
                    ru1a = max(base_1[:,a2]) - base_1[a1,a2]
                    ru1b = max(base_1[:,b2]) - base_1[b1,b2]
                    ru2a = max(base_2[a1]) - base_2[a1,a2]
                    ru2b = max(base_2[b1]) - base_2[b1,b2]
                    rsum1 = base_1[a1,a2] + base_1[b1,b2]
                    rsum2 = base_2[a1,a2] + base_2[b1,b2]
                    ir1 = rsum1 >= 2*vsec1 - tol
                    ir2 = rsum2 >= 2*vsec2 - tol
                    if ir1 and ir2:
                        ls = [(a1, a2),(b1, b2)]
                        ls.sort()
                        ir_targets.add(tuple(ls))
                    enf1 = rsum1/2 >= vsec1 + max(ru1a, ru1b) - tol
                    enf2 = rsum2/2 >= vsec2 + max(ru2a, ru2b) - tol
                    if enf1 and enf2:
                        ls = [(a1, a2),(b1, b2)]
                        ls.sort()
                        enf_targets.add(tuple(ls))
    return list(ir_targets), list(enf_targets)


def expert_br1(t, s, rg, inst, p_ix=0):
    '''
    Manipulator's best response expert
    '''
    if t % MANI_EPOCH == 0:
        inst.br_pol[p_ix] = mbrl(rg, inst.sNsa[1-p_ix], rg.stP,
          p_ix=p_ix, ess=True)[1]
        action = np.random.choice(np.arange(rg.A),
          p=inst.br_pol[p_ix][state_map(s, rg)])
    return action, 1


def nsa_range(states, actions, num_act, t1, t2, p_ix=0):
    '''
    Computes a history matrix for [t1, t2]
    '''
    Nsa = np.zeros((len(states), num_act))
    for t in range(t1, t2+1):
        Nsa[states[t], actions[p_ix][t]] += 1
    return Nsa


def follow_error(t, rg, inst, p_ix=0):
    '''
    Equivalent to B(tau) in the paper
    '''
    h = t - inst.started[p_ix]
    t1 = (rg.K + 2*rg.K*LAG_CONST*inst.time_since/rg.eps + 2*rg.K**2/rg.eps)/h
    t2 = rg.K*TUNING_FACTOR*opt_ql_error(t, rg, inst, p_ix)*h**(1/2)/rg.eps
    t3 = TUNING_FACTOR*(6*rg.K/rg.eps + 1)*(np.log(rg.tee/rg.delta)/2)**(1/2)
    return t1 + (t2 + t3)/(h**(1/2))


def opt_ql_error(t, rg, inst, p_ix=0):
    '''
    Q-learning avg regret
    '''
    return 33*SPAN_EST*(rg.S*rg.A*np.log(4*rg.tee**2/rg.delta)/(t-inst.started[p_ix]))**(1/3)


def expert_rep_qlearn(t, s, rg, inst, y, p_ix=0, erg=False, eee=False):
    '''
    Conditional Follower phi_F
    '''
    if (erg or eee) and ((t - inst.started[p_ix]) % inst.subepoch**2 > 0):
        if (((t - inst.started[p_ix]) % inst.subepoch**2) % inst.subepoch == 0 and
          not inst.punishing[p_ix]):
            rew_self = np.mean(inst.reward_list[p_ix][(inst.started[p_ix]+rg.K):])
            if VERBOSE_ALGOS:
                print(opt_ql_error(t, rg, inst, p_ix))
            if rew_self < rg.fair_val[p_ix] - opt_ql_error(t, rg, inst, p_ix):      
                inst.punishing[p_ix] = True
                if VERBOSE_ALGOS:
                    print('pun')
    qs = np.max(inst.Qtab[p_ix], axis=1).reshape(inst.Qtab[p_ix].shape[0], 1)
    mat = (inst.Qtab[p_ix] == qs).astype(int)
    polsum = mat.sum(axis=1)
    pol = mat / (polsum[:,None])
    action = np.random.choice(np.arange(rg.A), p=pol[s])
    return action, 1


def expert_maximin(t, s, rg, inst, p_ix=0, y=0.01, z=0.01):
    '''
    Maximin without exploitation check
    '''
    action = np.random.choice(np.arange(rg.A), p=rg.safe[p_ix][s])
    return action, 1


def mm_error(t, rg, inst, p_ix=0):
    '''
    Hoeffding term for Maximin
    '''
    return (np.log(rg.tee/rg.delta)/(2*(t-inst.started[p_ix])))**(1/2)


def expert_rep_maximin(t, s, rg, inst, y, p_ix=0, erg=False, eee=False):
    '''
    Conditional Maximin phi_M
    '''
    action = np.random.choice(np.arange(rg.A), p=rg.safe[p_ix][s])
    if (erg or eee) and ((t - inst.started[p_ix]) % inst.subepoch**2 > 0):
        if (((t - inst.started[p_ix]) % inst.subepoch**2) % inst.subepoch == 0 and
          not inst.punishing[p_ix]):
            rew_other = np.mean(inst.reward_list[1-p_ix][(inst.started[p_ix]+rg.K):])
            if rew_other > rg.vebs[1-p_ix] - y + mm_error(t-rg.K, rg, inst, p_ix):
                inst.punishing[p_ix] = True
    return action, 1


'''
Algorithm functions

Note that some of these functions have unused arguments because
a generic call to an algorithm function is used in the game
simulations.
'''


def algo_fictplay(t, s, rg, inst, p_ix=0, init=False, name=False):
    '''
    Fictitious Play
    '''
    if init:
        return 1
    if name:
        return 'FP'
    mod = inst.action_dist[1-p_ix]
    util = rg.rewards_1[0].dot(mod) if p_ix == 0 else mod.dot(rg.rewards_2[0])
    action = np.argmax(util)
    return action, 1
    
    
def algo_spp(t, s, rg, inst, p_ix=0, init=False, name=False):
    '''
    S++
    '''
    if init:
        return 1
    if name:
        return 'S++'
    # Safety override check
    weight = 1
    if t > 0 and inst.running_avg[p_ix] + inst.C/t < rg.vsec[p_ix]:
        inst.unsafe[p_ix] = True
    try:
        a1 = inst.actions[0][-1]
        a2 = inst.actions[1][-1]
    except:
        a1 = 0
        a2 = 0
    oppol, inst.br_pol[p_ix] = mbrl(rg, inst.sNsa[1-p_ix], rg.stP,
      p_ix=p_ix, ess=True)
    if (a1, a2) in inst.histC and t > 0:
        # At end of an epoch, update expert
        ri = inst.laff_avg[p_ix]
        # Updating record of last used expert
        inst.asp[p_ix] = ri + inst.lam**(t-inst.started[p_ix]+1)*(inst.asp[p_ix] - ri)
        # Update BR and potentials
        inst.update_potential(rg, inst.br_pol[p_ix], oppol,
          inst.n_leaders + inst.n_followers, p_ix=p_ix)
        loc_opp, pol1 = mbrl(rg, inst.loc_sNsa[1-p_ix], rg.stP, p_ix=p_ix,
          ess=True)
        val = mrp_value(rg.s_rewards_2[0] if p_ix else rg.s_rewards_1[0],
          pol1, loc_opp, rg.stP)
        for ind in range(inst.n_leaders,
          inst.n_leaders + inst.n_followers):
            # This check adds a few seconds of runtime.
            if val == inst.s_experts[p_ix][ind][4]:
                inst.potens[p_ix][ind] = inst.s_experts[p_ix][ind][4]
            else:
                inst.s_experts[p_ix][ind][1] = 0
        # Filter to high-potential experts
        inst.high_pot[p_ix] = np.where(inst.potens[p_ix] >= inst.asp[p_ix])[0]
        if not inst.high_pot[p_ix].size: 
            # FP is the default expert if none have high potential
            inst.high_pot[p_ix] = np.array([inst.n_leaders + inst.n_followers])
        # Safety override
        if inst.unsafe[p_ix]:
            inst.high_pot[p_ix] = np.array([inst.n_leaders + inst.n_followers + 1])
        # Best response override
        if inst.n_leaders + inst.n_followers in inst.high_pot[p_ix]:
            best_rec = np.max(inst.records[p_ix][inst.high_pot[p_ix]]) - 1e-8
            if inst.records[p_ix][inst.n_leaders + inst.n_followers] >= best_rec:
                inst.high_pot[p_ix] = np.array([inst.n_leaders + inst.n_followers])
        # If last reward was less than current asp, some probability of switching to a 
        # new high-potential expert
        if np.random.rand() > min(1, (ri/inst.asp[p_ix])**(inst.eplen)):
            inst.exp_ind[p_ix] = np.random.choice(inst.high_pot[p_ix])
        inst.laff_avg[p_ix] = 0
        inst.histC = set()
        inst.started[p_ix] = t
    inst.histC.add((a1, a2))

    if inst.exp_ind[p_ix] < inst.n_leaders:
        # Leader
        target = inst.s_experts[p_ix][inst.exp_ind[p_ix]][0]
        gs = rg.gsfp[state_map(s, rg)]
        if t > 0:
            # Update guilt
            last_assoc_act = rg.geom_states[s][(2-p_ix)*rg.K-1]
            if not (last_assoc_act == target[(t-1) % 2][1-p_ix] and
              inst.guilts[p_ix][inst.exp_ind[p_ix]] == 0):
                # i.e. other player deviated or has some guilt
                delt = 0.1 if inst.guilts[p_ix][inst.exp_ind[p_ix]] == 0 else 0
                otherval = inst.s_experts[p_ix][inst.exp_ind[p_ix]][-1]
                stepval = inst.reward_list[1-p_ix][-1]
                inst.guilts[p_ix][inst.exp_ind[p_ix]] = max(0,
                  inst.guilts[p_ix][inst.exp_ind[p_ix]] + stepval - otherval + delt)
        if inst.guilts[p_ix][inst.exp_ind[p_ix]] <= 0:
            try:
                ind = 1-target.index(gs) if t > 0 else 0
            except:
                # Picks up order of the targets as if they were always played
                ind = t % 2
            action = target[ind][p_ix]
        else:
            action = np.random.choice(np.arange(rg.A), p=rg.attack[p_ix])
    elif inst.exp_ind[p_ix] < inst.n_leaders + inst.n_followers:
        # Follower
        action = np.random.choice(np.arange(rg.A),
          p=inst.fol_pols[p_ix][inst.exp_ind[p_ix]][state_map(s, rg)])
    elif inst.exp_ind[p_ix] == inst.n_leaders + inst.n_followers:
        # Fict Play
        action = np.random.choice(np.arange(rg.A),
          p=inst.br_pol[p_ix][state_map(s, rg)])
    else:
        # Maximin
        action = np.random.choice(np.arange(rg.A), p=rg.safe[p_ix][s])
    return action, weight


def algo_Qlearn_eps(t, s, rg, inst, p_ix=0, init=False, name=False):
    '''
    Simple Q-learner
    '''
    if init:
        return 1
    if name:
        return 'Eps-Greedy Q'
    eps = 1/(10+(t+1)/10)
    qs = np.max(inst.Qtab[p_ix], axis=1).reshape(inst.Qtab[p_ix].shape[0], 1)
    mat = (inst.Qtab[p_ix] == qs).astype(int)
    m = inst.Qtab[p_ix].shape[1]
    arr = mat*(1-eps) + (np.ones_like(inst.Qtab[p_ix]) - mat)*eps/(m - 1)
    polsum = arr.sum(axis=1)
    pol = arr / (polsum[:,None])
    action = np.random.choice(np.arange(rg.A), p=pol[s])
    return action, 1


def algo_bully_godfather(t, s, rg, inst, p_ix=0, eps=0,
  init=False, name=False):
    '''
    Bully
    '''
    if name:
        return 'Bully Godfather'
    weight = rg.prob_bul[p_ix][0]
    if init:
        return weight
    mat = rg.trub[p_ix]
    arr = mat*(1-eps) + (np.ones_like(mat) - mat)*eps/(mat.shape[1] - 1)
    polsum = arr.sum(axis=1)
    pol = arr / (polsum[:,None])
    action = np.random.choice(np.arange(rg.A), p=pol[s])
    return action, weight 


def algo_manipulator_bully(t, s, rg, inst, p_ix=0, p=0.00005,
  eps0=0.025, eps1=0.025, eps3=0.025, init=False, name=False):
    '''
    Manipulator
    '''
    if name:
        return 'Bully Manipulator'
    temp_ei = -1
    if init:
        return 1
    if t < TAU1:
        inst.exp_ind[p_ix] = 2
        inst.strategy[p_ix] = 2
    if TAU1 <= t < TAU1 + TAU2:
        # Checking if less than bully value; note that this is
        # different from LAFF's bully value (see RGInstance)
        if inst.rolling_avg[p_ix] < inst.bully_value[p_ix] - eps1:
            if np.random.rand() < p:
                inst.exp_ind[p_ix] = 3
                inst.strategy[p_ix] = 3
    if t == TAU1 + TAU2:
        Nsa1 = nsa_range(inst.game_states, inst.actions, rg.A, 0, TAU1,
          p_ix=1-p_ix)
        Nsa2 = nsa_range(inst.game_states, inst.actions, rg.A, t-TAU1, t-1,
          p_ix=1-p_ix)
        oppol1 = Nsa1 / Nsa1.sum(axis=1)[:,None]
        oppol1 = np.nan_to_num(oppol1, nan=1/oppol1.shape[1])
        oppol2 = Nsa2 / Nsa2.sum(axis=1)[:,None]
        oppol2 = np.nan_to_num(oppol2, nan=1/oppol2.shape[1])
        if policynorm(oppol1, oppol2) < eps3:
            inst.exp_ind[p_ix] = 3
            inst.opp_stationary[p_ix] = 1
        elif (inst.exp_ind[p_ix] == 2 and
          inst.running_avg[p_ix] > inst.bully_value[p_ix] - eps1):
            inst.exp_ind[p_ix] = 2
        else:
            inst.exp_ind[p_ix] = 3
    if t >= TAU1 + TAU2 + TAU1*inst.opp_stationary[p_ix]:
        if inst.opp_stationary[p_ix]:
            inst.opp_stationary[p_ix] = 0
            Nsa1 = nsa_range(inst.game_states, inst.actions, rg.A, 0, TAU1,
              p_ix=1-p_ix)
            Nsa2 = nsa_range(inst.game_states, inst.actions, rg.A, t-TAU1, t-1,
              p_ix=1-p_ix)
            oppol1 = Nsa1 / Nsa1.sum(axis=1)[:,None]
            oppol1 = np.nan_to_num(oppol1, nan=1/oppol1.shape[1])
            oppol2 = Nsa2 / Nsa2.sum(axis=1)[:,None]
            oppol2 = np.nan_to_num(oppol2, nan=1/oppol2.shape[1])
            if policynorm(oppol1, oppol2) >= eps3:
                inst.exp_ind[p_ix] = inst.strategy[p_ix]
        if inst.rolling_avg[p_ix] < rg.vsec[p_ix] - eps0:
            temp_ei = 1

    # Take action and weight based on selected strategy
    if inst.exp_ind[p_ix] == 2 and temp_ei != 1:
        # Bully Godfather
        target = inst.s_experts[p_ix][inst.bully_ind[p_ix]][0]
        gs = rg.gsfp[state_map(s, rg)]
        last_assoc_act = rg.geom_states[s][(2-p_ix)*rg.K-1]
        if t > 0 and not last_assoc_act == target[(t-1) % 2][1-p_ix]:
            # i.e. other player deviated
            action = np.random.choice(np.arange(rg.A), p=rg.attack[p_ix])
        else:
            try:
                ind = 1-target.index(gs) if t > 0 else 0
            except:
                # Picks up order of the targets as if they were always played
                ind = t % 2
            action = target[ind][p_ix]
        weight = 1
    if inst.exp_ind[p_ix] == 3 and temp_ei != 1:
        # Follower
        action, weight = expert_br1(t, s, rg, inst, p_ix=p_ix)
    if temp_ei == 1:
        # Maximin
        action, weight = expert_maximin(t, s, rg, inst, p_ix=p_ix)
    if t % 10000 == 0:
        if VERBOSE_ALGOS:
            print(inst.rolling_avg[p_ix])
    return action, weight


def algo_gen_godfather(t, s, rg, inst, p_ix=0, eps=0, init=False, name=False):
    '''
    Forgiving TFT
    '''
    if name:
        return 'Generous Godfather'
    weight = rg.prob_ebs[p_ix][0]
    if init:
        return weight
    mat = rg.generous[p_ix]
    arr = mat*(1-eps) + (np.ones_like(mat) - mat)*eps/(mat.shape[1] - 1)
    polsum = arr.sum(axis=1)
    pol = arr / (polsum[:,None])
    action = np.random.choice(np.arange(rg.A), p=pol[s])
    return action, weight


def algo_laff(t, s, rg, inst, p_ix=0, y=1, init=False, name=False):
    '''
    LAFF
    '''
    if name:
        return 'LAFF'
    if init:
        return 0.5
    temp_ei = 0 if inst.punishing[p_ix] else -1
    if not inst.tried_bul_follow[p_ix]:
        inst.exp_ind[p_ix] = 3
    elif not inst.rejected_bully[p_ix]:
        inst.exp_ind[p_ix] = 2
    elif not inst.tried_egal_follow[p_ix]:
        inst.exp_ind[p_ix] = 3
    elif not inst.rejected_egal[p_ix]:
        inst.exp_ind[p_ix] = 0
    
    # Checking whether to switch experts
    if (t - inst.started[p_ix]) % inst.subepoch**2 == 0:
        if t - inst.started[p_ix] > 0:
            err = follow_error(t, rg, inst, p_ix)
            if not inst.tried_bul_follow[p_ix]:
                if VERBOSE_ALGOS:
                    print(err)
                if inst.laff_avg[p_ix] < rg.bul[p_ix] - err:
                    inst.exp_ind[p_ix] = 2
                    inst.time_since = t - inst.started[p_ix]
                    inst.started[p_ix] = t
                    inst.laff_avg[p_ix] = 0
                    inst.tried_bul_follow[p_ix] = True
            elif not inst.rejected_bully[p_ix]:
                if VERBOSE_ALGOS:
                    print(err)
                if inst.laff_avg[p_ix] < rg.bul[p_ix] - err:
                    inst.rejected_bully[p_ix] = True
                    inst.exp_ind[p_ix] = 3
                    inst.started[p_ix] = t
                    inst.laff_avg[p_ix] = 0
                    inst.time_since = 0
                    if VERBOSE_ALGOS:
                        print(t, 'Bully rejected')
            elif not inst.tried_egal_follow[p_ix]:
                if VERBOSE_ALGOS:
                    print(err)
                if inst.laff_avg[p_ix] < rg.vebs[p_ix] - err:
                    inst.exp_ind[p_ix] = 0
                    inst.time_since = t - inst.started[p_ix]
                    inst.started[p_ix] = t
                    inst.laff_avg[p_ix] = 0
                    inst.tried_egal_follow[p_ix] = True
            elif not inst.rejected_egal[p_ix]:
                if VERBOSE_ALGOS:
                    print(err)
                if inst.laff_avg[p_ix] < rg.vebs[p_ix] - err:
                    inst.rejected_egal[p_ix] = True
                    inst.exp_ind[p_ix] = 3
                    inst.started[p_ix] = t
                    inst.laff_avg[p_ix] = 0
                    inst.time_since = 0
                    if VERBOSE_ALGOS:
                        print(t, 'Egal rejected')
            else:
                if (inst.exp_ind[p_ix] != 1 and
                  inst.laff_avg[p_ix] < rg.vsec[p_ix] - err):
                    inst.exp_ind[p_ix] = 1
                    inst.punishing[p_ix] = False

    if inst.exp_ind[p_ix] == 0 or (temp_ei == 0 and inst.exp_ind[p_ix] in [1, 3]):
        # Egal Godfather
        if (t - inst.started[p_ix]) % inst.subepoch**2 < rg.K + 1:
            # Play nice early on
            if rg.enforceable[p_ix]:
                action = rg.targets_e[p_ix][rg.geom_states[s][-1-(rg.K+1)*(1-p_ix)]][p_ix]
            else:
                action = np.random.choice(np.arange(rg.A), p=rg.safe[p_ix][s])
        else:
            action = np.random.choice(np.arange(rg.A), p=rg.godf[p_ix][s])
        weight = rg.prob_ebs[p_ix][0]
    elif inst.exp_ind[p_ix] == 2 and temp_ei != 1:
        # Bully Godfather
        if (t - inst.started[p_ix]) % inst.subepoch**2 < rg.K + 1:
            # Play nice early on
            if rg.enforceable[p_ix]:
                action = rg.targets_b[p_ix][rg.geom_states[s][-1-(rg.K+1)*(1-p_ix)]][p_ix]
            else:
                action = np.random.choice(np.arange(rg.A), p=rg.safe[p_ix][s])
        else:
            action = np.random.choice(np.arange(rg.A), p=rg.trub[p_ix][s])
        weight = rg.prob_bul[p_ix][0]
    elif inst.exp_ind[p_ix] == 3 and temp_ei != 1:
        # Follower
        action, weight = expert_rep_qlearn(t, s, rg, inst, y, p_ix=p_ix, eee=True)
    elif inst.exp_ind[p_ix] == 1:
        # Maximin
        action, weight = expert_rep_maximin(t, s, rg, inst,  y, p_ix=p_ix, eee=True)
    return action, weight


def algo_mqubed(t, s, rg, inst, p_ix=0, init=False, name=False):
    '''
    M-Qubed
    '''
    if name:
        return 'M-Qubed'
    if init:
        return 1
    inst.danger[p_ix] = inst.danger[p_ix] or (inst.loss[p_ix] >= inst.tol)
    beta = 1 if inst.danger[p_ix] else (np.max(inst.loss[p_ix], 0)/inst.tol)**10
    eta = mq_eta(inst.Nsa[p_ix])
    inst.update_mq_pol(rg, beta, eta, p_ix=p_ix)
    action = np.random.choice(np.arange(rg.A), p=inst.mq_pol[p_ix][s])
    return action, 1


class RepeatedGame:
    '''
    Container for properties of a repeated game
    '''
    def __init__(self, base_1, base_2, tee, K=1, eps=0.05, delta=0.005,
          fair_val_1=-1, fair_val_2=-1):
        self.eps = eps
        self.base_1 = base_1
        self.base_2 = base_2
        self.tee = tee
        self.K = K
        self.A = base_1.shape[0]
        self.S = 2**(2*(K+1))*self.A**(2*K)
        self.geom_states = list(itertools.product(list(range(self.A)), repeat=2*K))
        for _ in range(2*(K+1)):
            self.geom_states = ([tuple(list(i) + [0]) for i in self.geom_states] +
              [tuple(list(i) + [1]) for i in self.geom_states])

        # Also constructing states for past 1 joint action, no randomization
        # (for S++ fictitious play)
        self.sfp = self.A**2
        self.gsfp = list(itertools.product(list(range(self.A)), repeat=2))
        self.stP = simple_tp(self.sfp, self.A, self.gsfp)

        rew = np.zeros((self.S, self.A, self.A))
        self.rewards_1 = rew.copy()
        self.rewards_2 = rew.copy()
        self.rewards_1[:], self.rewards_2[:] = game_norm(base_1, base_2)

        # Reward representations for S++
        srew = np.zeros((self.sfp, self.A, self.A))
        self.s_rewards_1 = srew.copy()
        self.s_rewards_2 = srew.copy()
        self.s_rewards_1[:], self.s_rewards_2[:] = game_norm(base_1, base_2)

        # Constructing experts for LAFF and important baseline values
        self.godf = {0: np.zeros(1), 1: np.zeros(1)}
        self.trub = {0: np.zeros(1), 1: np.zeros(1)}
        self.safe = {0: np.zeros(1), 1: np.zeros(1)}
        self.generous = {0: np.zeros(1), 1: np.zeros(1)}
        self.vebs = {0: 0, 1: 0}
        self.bul = {0: 0, 1: 0}
        self.kind = {0: 0, 1: 0}
        self.vsec = {0: 0, 1: 0}
        self.dum = {0: 0, 1: 0}
        self.attack = {0: punish(self.rewards_1[0], self.rewards_2[0], 0),
                       1: punish(self.rewards_1[0], self.rewards_2[0], 1)}
        self.enforceable = {0: bargain(self.rewards_1[0], self.rewards_2[0],
          eps=eps, p_ix=0, K=K) is not None, 1: bargain(self.rewards_1[0],
          self.rewards_2[0], eps=eps, p_ix=1, K=K) is not None}
        self.safe[0], self.vsec[0] = maximin(self.rewards_1[0], self.rewards_2[0],
          self.S, self.A)
        self.safe[1], self.vsec[1] = maximin(self.rewards_1[0], self.rewards_2[0],
          self.S, self.A, p_ix=1)
        self.godf[0], self.vebs[0], we1 = godfather(self, solution='ebs', eps=eps)
        self.godf[1], self.vebs[1], we2 = godfather(self, p_ix=1, solution='ebs',
          eps=eps)
        self.trub[0], self.bul[0], wb1 = godfather(self, eps=eps)
        self.trub[1], self.bul[1], wb2 = godfather(self, p_ix=1, eps=eps)
        self.generous[0] = godfather(self, solution='ebs', eps=eps,
          generosity=0.8)[0]
        self.generous[1] = godfather(self, p_ix=1, solution='ebs', eps=eps,
          generosity=0.8)[0]
        self.fair_val = {0: self.vebs[0] if fair_val_1 == -1 else fair_val_1,
          1: self.vebs[1] if fair_val_2 == -1 else fair_val_2}
        try:
            target_1_1_e, target_1_2_e = bargain(self.rewards_1[0],
              self.rewards_2[0], eps=eps, p_ix=0, K=K)[:2]
            target_2_1_e, target_2_2_e = bargain(self.rewards_1[0],
              self.rewards_2[0], eps=eps, p_ix=1, K=K)[:2]
            target_1_1_b, target_1_2_b = bargain(self.rewards_1[0],
              self.rewards_2[0], eps=eps, p_ix=0, solution='bully', K=K)[:2]
            target_2_1_b, target_2_2_b = bargain(self.rewards_1[0],
              self.rewards_2[0], eps=eps, p_ix=1, solution='bully', K=K)[:2]
        except:
            target_1_1_e, target_1_2_e = ((0,0), (0,0))
            target_2_1_e, target_2_2_e = ((0,0), (0,0))
            target_1_1_b, target_1_2_b = ((0,0), (0,0))
            target_2_1_b, target_2_2_b = ((0,0), (0,0))
        self.targets_e = {0: [target_1_1_e, target_1_2_e],
                              1: [target_2_1_e, target_2_2_e]}
        self.targets_b = {0: [target_1_1_b, target_1_2_b],
                              1: [target_2_1_b, target_2_2_b]}

        # Constructing experts for S++
        self.ir_targets, self.enf_targets = construct_targets(self.rewards_1[0],
          self.rewards_2[0])

        # Weights from bargaining solutions
        self.prob_ebs = {0: [we1, 1-we1], 1: [we2, 1-we2]}
        self.prob_bul = {0: [wb1, 1-wb1], 1: [wb2, 1-wb2]}
        
        self.tPs = {}
        self.tPs[(self.prob_ebs[0][0], self.prob_ebs[0][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_ebs[0], self.prob_ebs[0])
        self.tPs[(self.prob_ebs[1][0], self.prob_ebs[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_ebs[1], self.prob_ebs[1])
        self.tPs[(self.prob_bul[0][0], self.prob_bul[0][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_bul[0], self.prob_bul[0])
        self.tPs[(self.prob_bul[1][0], self.prob_bul[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_bul[1], self.prob_bul[1])
        # Cases where players have different randomization biases
        self.tPs[(self.prob_ebs[0][0], self.prob_ebs[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_ebs[0], self.prob_ebs[1])
        self.tPs[(self.prob_ebs[0][0], self.prob_bul[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_ebs[0], self.prob_bul[1])
        self.tPs[(self.prob_bul[0][0], self.prob_ebs[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_bul[0], self.prob_ebs[1])
        self.tPs[(self.prob_bul[0][0], self.prob_bul[1][0])] = transition(self.S,
          self.A, self.geom_states, K, self.prob_bul[0], self.prob_bul[1])

        self.delta = delta


class RGInstance:
    '''
    Includes data and state variables of an instance of a repeated game
    '''
    def __init__(self, rg, algo_1, algo_2, lr=LEARN, lam=0.99, zeta=0.05,
      gam=0.95):
        self.subepoch = floor(rg.tee**(1/4))

        self.Nsa = {0: np.zeros((rg.S, rg.A)), 1: np.zeros((rg.S, rg.A))}
        self.reward_list = {0: [], 1: []}
        self.running_avg = {0: 0, 1: 0}
        self.rolling_avg = {0: 0, 1: 0}
        self.laff_avg = {0: 0, 1: 0}
        self.init_w1 = algo_1(0, 0, rg, None, p_ix=0, init=True)
        self.init_w2 = algo_2(0, 0, rg, None, p_ix=1, init=True)
        self.action_dist = {0: np.zeros(rg.A), 1: np.zeros(rg.A)}
        if (self.init_w1, self.init_w2) not in rg.tPs.keys():
            rg.tPs[(self.init_w1, self.init_w2)] = transition(rg.S, rg.A, rg.geom_states,
              rg.K, (self.init_w1, 1-self.init_w1), (self.init_w2, 1-self.init_w2))
        self.game_states = []
        self.actions = {0: [], 1: []}
        self.exp_ind = {0: np.random.choice(4), 1: np.random.choice(4)}
        # Important: agents *plan* as if current tPs will be constant
        self.s = 0
        self.horizon = ceil(rg.tee**(1/2))
        self.punishing = {0: False, 1: False}
        self.started = {0: 0, 1: 0}
        # Below initialization is important for M-Qubed
        self.Qtab = {0: np.ones((rg.S, rg.A))/(1-gam), 1: np.ones((rg.S, rg.A))/(1-gam)}
        self.lr = lr
        self.gam = gam
        self.br_pol = {0: mbrl(rg, self.Nsa[1], rg.tPs[(self.init_w1,
          self.init_w2)])[1], 1: mbrl(rg, self.Nsa[0], rg.tPs[(self.init_w1,
          self.init_w2)])[1]}
        
        # S++ objects
        self.asp = {0: rg.vebs[0], 1: rg.vebs[1]}
        self.lam = lam
        self.n_leaders = len(rg.enf_targets)
        self.n_followers = len(rg.ir_targets)
        # [target, potential, record, number of epochs used,
        # value for other if leader else init_potential]
        self.s_experts, self.fol_pols, self.guilts = make_spp_experts(rg)
        self.eplen = TAU
        self.high_pot = {0: np.arange(self.n_leaders + self.n_followers + 2),
          1: np.arange(self.n_leaders + self.n_followers + 2)}
        self.unsafe = {0: False, 1: False}
        self.C = 100
        # histories for the S++ model, i.e. conditioning just on past joint action
        self.sNsa = {0: np.zeros((rg.sfp, rg.A)), 1: np.zeros((rg.sfp, rg.A))}
        self.loc_sNsa = {0: np.zeros((rg.sfp, rg.A)), 1: np.zeros((rg.sfp, rg.A))}
        self.potens = {0: np.array([v[1] for v in self.s_experts[0].values()]),
          1: np.array([v[1] for v in self.s_experts[1].values()])}
        self.records = {0: np.ones(self.n_followers + self.n_leaders + 2),
                        1: np.ones(self.n_followers + self.n_leaders + 2)}
        self.histC = set()
        self.spp_runavg = {0: 0, 1: 0}

        # Unexploitable Fair Manipulator
        self.rejected_bully = {0: False, 1: False}
        self.rejected_egal = {0: False, 1: False}
        self.tried_bul_follow = {0: False, 1: False}
        self.tried_egal_follow = {0: False, 1: False}

        # M-Qubed
        self.zeta = zeta
        self.tol = 500*rg.A*rg.S # risk tolerance
        self.alpha = {0: (1-(zeta/(1-rg.vsec[0]+zeta))**(zeta*rg.A*rg.S/self.tol))/(1-gam),
          1: (1-(zeta/(1-rg.vsec[1]+zeta))**(zeta*rg.A*rg.S/self.tol))/(1-gam)}
        self.mq_pol = {0: np.ones((rg.S, rg.A)) / rg.A,
          1: np.ones((rg.S, rg.A)) / rg.A}
        self.loss = {0: 0, 1: 0}
        self.danger = {0: False, 1: False}
        self.st_tol = 0.01 # "small margin" for S* set

        self.time_since = 0

        # Manipulator
        self.opp_stationary = {0: 0, 1: 0}
        self.strategy = {0: 0, 1: 0}
        max_val_1 = -1
        max_val_2 = -1
        self.bully_ind = {0: 0, 1: 0}
        self.bully_value = {0: 0, 1: 0}
        for i in range(self.n_leaders):
            if self.s_experts[0][i][1] > max_val_1:
                max_val_1 = self.s_experts[0][i][1]
                self.bully_value[0] = max_val_1
                self.bully_ind[0] = i
            if self.s_experts[1][i][1] > max_val_2:
                max_val_2 = self.s_experts[1][i][1]
                self.bully_value[1] = max_val_2
                self.bully_ind[1] = i

    def update_potential(self, rg, pol1, pol2, ind, p_ix=0):
        rews = rg.s_rewards_2[0] if p_ix else rg.s_rewards_1[0]
        self.potens[p_ix][ind] = mrp_value(rews, pol1, pol2, rg.stP)

    def update_record(self, experts, ri, p_ix=0, ess=False):
        x = int(ess)
        if experts[p_ix][self.exp_ind[p_ix]][2+x] == 1:
            self.records[p_ix][self.exp_ind[p_ix]] = ri
        else:
            rover = self.records[p_ix][self.exp_ind[p_ix]]
            num = experts[p_ix][self.exp_ind[p_ix]][2+x]
            self.records[p_ix][self.exp_ind[p_ix]] = (rover*(num-1) + ri)/num

    def update_mq_pol(self, rg, beta, eta, p_ix=0):
        # By default, set STOPPING = False. This makes M-Qubed faster, and
        # based on Crandall and Goodrich (2010) this doesn't come at a 
        # noticeable performance difference.
        if STOPPING:
            best = self.Qtab[p_ix] >= self.Qtab[p_ix].max() - self.st_tol
            Sstar = np.unique(np.where(best)[0])
            Sprev = np.unique(self.game_states[-rg.S:])
            nonempty = bool(np.intersect1d(Sstar, Sprev).size)
        # Eq 10 of C+G
        Qmask = self.Qtab[p_ix].max(axis=1) > rg.vsec[p_ix] / (1-self.gam)
        greedy = distribut((self.Qtab[p_ix] == np.amax(self.Qtab[p_ix],
          axis=1, keepdims=True)).astype(int))
        # Eq 12 of C+G
        cautious = rg.safe[p_ix] if self.danger[p_ix] else greedy
        br = rg.safe[p_ix] + (greedy - rg.safe[p_ix])*Qmask[:,None]
        # Eq 13 of C+G
        pistar = beta*cautious + (1-beta)*br
        # Eq 16 of C+G
        if STOPPING:
            if beta == 1 or nonempty:
                policy = pistar
            else:
                policy = (1-eta)*pistar + eta/(self.Qtab[p_ix].shape[1])
            self.mq_pol[p_ix] = distribut(policy)
        else:
            policy = (1-eta)*pistar + eta/(self.Qtab[p_ix].shape[1])
            self.mq_pol[p_ix] = distribut(policy)


def follower_policy(rg, target, p_ix=0):
    '''
    Constructs policy corresponding to a target solution,
    for an S++ Follower
    '''
    my_actions = np.unique(np.array(target)[:,p_ix])
    pol = np.zeros((rg.sfp, rg.A))
    for s, gs in enumerate(rg.gsfp):
        try:
            ind = target.index(gs)
            pol[s,target[1-ind][p_ix]] = 1
        except:
            pol[s] = np.isin(np.arange(rg.A), my_actions).astype(int)
            pol[s] = pol[s] / pol[s].sum()
    return pol


def mq_eta(Nsa):
    '''
    Hyperparameter for M-Qubed
    '''
    kap = np.max(np.sum(Nsa, axis=1))
    return 0.04*1000/(1000+kap)


def make_spp_experts(rg):
    '''
    Order:
    1) Leaders
    2) Followers
    3) Fict Play
    4) Maximin

    Potentials of (2) and (3) will vary over the game
    '''
    ind = 0
    exp1 = {}
    exp2 = {}
    folpol1 = {}
    folpol2 = {}
    guilts1 = {}
    guilts2 = {}
    for target in rg.enf_targets:
        exp1[ind] = [target,
          (rg.rewards_1[0][target[0]] + rg.rewards_1[0][target[1]])/2,
          1, 0, (rg.rewards_2[0][target[0]] + rg.rewards_2[0][target[1]])/2]
        exp2[ind] = [target,
          (rg.rewards_2[0][target[0]] + rg.rewards_2[0][target[1]])/2,
          1, 0, (rg.rewards_1[0][target[0]] + rg.rewards_1[0][target[1]])/2]
        guilts1[ind] = 0
        guilts2[ind] = 0
        ind += 1
    for target in rg.ir_targets:
        exp1[ind] = [target,
          (rg.rewards_1[0][target[0]] + rg.rewards_1[0][target[1]])/2,
          1, 0, (rg.rewards_1[0][target[0]] + rg.rewards_1[0][target[1]])/2]
        exp2[ind] = [target,
          (rg.rewards_2[0][target[0]] + rg.rewards_2[0][target[1]])/2,
          1, 0, (rg.rewards_2[0][target[0]] + rg.rewards_2[0][target[1]])/2]
        folpol1[ind] = follower_policy(rg, target, p_ix=0)
        folpol2[ind] = follower_policy(rg, target, p_ix=1)
        ind += 1
    exp1[ind] = [None, 1, 1, 0]
    exp2[ind] = [None, 1, 1, 0]
    exp1[ind+1] = [None, rg.vsec[0], 1, 0]
    exp2[ind+1] = [None, rg.vsec[1], 1, 0]
    return {0: exp1, 1: exp2}, {0: folpol1, 1: folpol2}, {0: guilts1, 1: guilts2}


def game_step(t, s, a1, a2, rg, inst, tPs, algo_1, algo_2):
    '''
    One time step in RG
    '''
    inst.game_states.append(s)
    inst.actions[0].append(a1)
    inst.actions[1].append(a2)
    inst.Nsa[0][s,a1] += 1
    inst.Nsa[1][s,a2] += 1
    inst.sNsa[0][state_map(s, rg),a1] += 1
    inst.sNsa[1][state_map(s, rg),a2] += 1
    # Incorporating line 17 from Crandall (2014)
    if np.sum(inst.loc_sNsa[0][state_map(s, rg)] > 0) > 1:
        inst.loc_sNsa[0] = np.zeros((rg.sfp, rg.A))
    inst.loc_sNsa[0][state_map(s, rg),a1] += 1
    if np.sum(inst.loc_sNsa[1][state_map(s, rg)] > 0) > 1:
        inst.loc_sNsa[1] = np.zeros((rg.sfp, rg.A))
    inst.loc_sNsa[1][state_map(s, rg),a2] += 1
    r1 = rg.rewards_1[s,a1,a2]
    r2 = rg.rewards_2[s,a1,a2]
    if algo_1 == algo_spp:
        inst.s_experts[0][inst.exp_ind[0]][3] += 1
        kap = inst.s_experts[0][inst.exp_ind[0]][3]
        inst.records[0][inst.exp_ind[0]] = (r1 +
          max(1/kap, 0.02)*(inst.records[0][inst.exp_ind[0]] - r1))
    if algo_2 == algo_spp:
        inst.s_experts[1][inst.exp_ind[1]][3] += 1
        kap = inst.s_experts[1][inst.exp_ind[1]][3]
        inst.records[1][inst.exp_ind[1]] = (r2 +
          max(1/kap, 0.02)*(inst.records[1][inst.exp_ind[1]] - r2))
    inst.reward_list[0].append(r1)
    inst.running_avg[0] = (r1 + t*inst.running_avg[0])/(t+1)
    inst.laff_avg[0] = ((r1 +
      (t - inst.started[0])*inst.laff_avg[0])/(t - inst.started[0] + 1))
    inst.reward_list[1].append(r2)
    inst.running_avg[1] = (r2 + t*inst.running_avg[1])/(t+1)
    inst.laff_avg[1] = ((r2 +
      (t - inst.started[1])*inst.laff_avg[1])/(t - inst.started[1] + 1))
    if algo_1 == algo_manipulator_bully:
        if t == MANI_H:
            inst.rolling_avg[0] = inst.running_avg[0]
        elif t > MANI_H:
            inst.rolling_avg[0] += (r1 - inst.reward_list[0][-(MANI_H+1)])/MANI_H
    if algo_2 == algo_manipulator_bully:
        if t == MANI_H:
            inst.rolling_avg[1] = inst.running_avg[1]
        elif t > MANI_H:
            inst.rolling_avg[1] += (r2 - inst.reward_list[1][-(MANI_H+1)])/MANI_H
    inst.loss[0] += rg.vsec[0] - r1
    inst.loss[1] += rg.vsec[1] - r2
    sp = np.random.choice(np.arange(rg.S), p=tPs[s,a1,a2])
    if not inst.punishing[0]:
        if algo_1 == algo_mqubed:
            # Does SARSA updates instead of Q-learning
            inst.Qtab[0][s,a1] += inst.alpha[0]*(r1 + inst.gam*inst.Qtab[0][sp].dot(inst.mq_pol[0][sp]) - inst.Qtab[0][s,a1])
        else:
            if algo_1 != algo_laff or inst.exp_ind[0] == 3:
                inst.Qtab[0][s,a1] += inst.lr/(t/100+10)*(r1 + inst.gam*np.max(inst.Qtab[0][sp]) - inst.Qtab[0][s,a1])
    if not inst.punishing[1]:
        if algo_2 == algo_mqubed:
            inst.Qtab[1][s,a2] += inst.alpha[1]*(r2 + inst.gam*inst.Qtab[1][sp].dot(inst.mq_pol[1][sp]) - inst.Qtab[1][s,a2])
        else:
            if algo_2 != algo_laff or inst.exp_ind[1] == 3:
                inst.Qtab[1][s,a2] += inst.lr/(t/100+10)*(r2 + inst.gam*np.max(inst.Qtab[1][sp]) - inst.Qtab[1][s,a2])
    return sp


def game_trial(rg, tee, algo_1, algo_2):
    '''
    Game of length tee between algo_1 and algo_2
    
    All algos will take the same inputs and give same types of outputs,
    though some may be unchanged if not used.
    Outputs: action, randomization weight
    '''
    inst = RGInstance(rg, algo_1, algo_2)
    s = inst.s
    computing_act_dist_1 = algo_1 == algo_fictplay
    computing_act_dist_2 = algo_2 == algo_fictplay
    if algo_1 == algo_spp:
        inst.br_pol[0] = np.ones((rg.sfp, rg.A)) / rg.A
    if algo_2 == algo_spp:
        inst.br_pol[1] = np.ones((rg.sfp, rg.A)) / rg.A
    for t in range(tee):
        # need to set tPs
        a1, weight1 = algo_1(t, s, rg, inst, p_ix=0)
        a2, weight2 = algo_2(t, s, rg, inst, p_ix=1)
        if computing_act_dist_2:
            inst.action_dist[0] *= t
            inst.action_dist[0][a1] += 1
            inst.action_dist[0] /= (t+1)
        if computing_act_dist_1:
            inst.action_dist[1] *= t
            inst.action_dist[1][a2] += 1
            inst.action_dist[1] /= (t+1)
        if (weight1, weight2) not in rg.tPs.keys():
            rg.tPs[(weight1, weight2)] = transition(rg.S, rg.A, rg.geom_states,
              rg.K, (weight1, 1-weight1), (weight2, 1-weight2))
        s = game_step(t, s, a1, a2, rg, inst, rg.tPs[(weight1, weight2)],
          algo_1, algo_2)
        if t % 1000 == 0:
            if VERBOSE_TRIAL:
                print(inst.exp_ind[0], inst.exp_ind[1], inst.punishing[0], inst.punishing[1],
                  'Cumulative:', str(np.round(inst.running_avg[0], 2)),
                  str(np.round(inst.running_avg[1], 2)))
    return inst


def game_norm(base_1, base_2):
    '''
    Normalizes game matrices to [0, 1]
    '''
    b1 = base_1 - np.min(base_1)
    r1 = b1 / np.max(b1)
    b2 = base_2 - np.min(base_2)
    r2 = b2 / np.max(b2)
    return r1, r2


def create_path(path):
    '''
    CREDIT: https://stackoverflow.com/questions/944536/efficient-way-of-creating-recursive-paths-python
    '''
    paths_to_create = []
    while not os.path.lexists(path):
        paths_to_create.insert(0, path)
        head,tail = os.path.split(path)
        if len(tail.strip())==0: # Just incase path ends with a / or \
            path = head
            head,tail = os.path.split(path)
        path = head

    for path in paths_to_create:
        os.mkdir(path)


def write_rewards(reward_list, game_name, alg1_name, alg2_name, itr, experiment_id):
    '''
    Saves results from one (game, alg1, alg2) combination

    "itr" keeps track of which trial it is

    experiment_id is some ID string I use for a full experiment
    '''
    path = os.getcwd() + '/' + game_name + '/' + alg1_name + '/' + alg2_name
    if not os.path.isdir(path):
        create_path(path)
    if not os.path.isdir(path + '/P1/' + experiment_id):
        create_path(path + '/P1/' + experiment_id)
    if not os.path.isdir(path + '/P2/' + experiment_id):
        create_path(path + '/P2/' + experiment_id)
    np.savetxt(path + '/P1/' + experiment_id + '/' + str(itr) + '.txt',
      reward_list[0])
    np.savetxt(path + '/P2/' + experiment_id + '/' + str(itr) + '.txt',
      reward_list[1])


def run_game(game_name, alg1_name, alg2_name, itr, tee, experiment_id):
    '''
    Trial for (game, alg1, alg2) combination
    '''
    rg = RepeatedGame(game_map[game_name][0], game_map[game_name][1], tee,
      eps=0.05, K=1)
    symmetric = np.all(rg.base_1 == rg.base_2.transpose())
    i = names_final_algo.index(alg1_name)
    j = names_final_algo.index(alg2_name)
    if symmetric and j < i:
        pass
    else:
        res = game_trial(rg, tee, algo_map[alg1_name],
          algo_map[alg2_name]).reward_list
        write_rewards(res, game_name, alg1_name, alg2_name, itr, experiment_id)


def experiment_trial(task_id):
    '''
    Converts integer task_id (from code below) to a (game, alg1, alg2, itr)
    combination
    '''
    combo = combos[task_id]
    game_name = game_names[combo[0]]
    alg1_name = names_final_algo[combo[2]]
    alg2_name = names_final_algo[combo[3]]
    itr = combo[1]
    np.random.seed(itr)
    experiment_id = 'test_B'
    run_game(game_name, alg1_name, alg2_name, itr, TIME, experiment_id)


#%%

final_algo_list = [algo_spp, algo_manipulator_bully, algo_mqubed, algo_bully_godfather,
                  algo_Qlearn_eps, algo_laff, algo_gen_godfather, algo_fictplay]
names_final_algo = [alg1(0, 0, None, None, name=True) for alg1 in final_algo_list]
algo_map = dict(zip(names_final_algo, final_algo_list))


#%%

'''
Game Periodic Table

3 Simple, 3 Dominated, 5 Temptation
'''

# (3, 2, 2) Stag Hunt
# (WW + Simple)
winwin1_s = np.array([[4, 1], [3, 2]])
winwin2_s = winwin1_s.transpose()
# (1, 3, 3) BOS (though with higher payoff for miscoordination at one spot
# than another)
# (Bi + Temptation)
biased1_s = np.array([[2, 3], [4, 1]])
biased2_s = biased1_s.transpose()
# (1, 6, 6) Deadlock (no symmetric games in Second-Best have enforceable EBS)
# (SB + Simple)
second1_s = np.array([[2, 1], [4, 3]])
second2_s = second1_s.transpose()
# (1, 2, 2) Chicken
# (Un + Temptation)
unfair1_s = np.array([[3, 2], [5, 1]]) # 5 instead of 4 so blended EBS
unfair2_s = unfair1_s.transpose()
# ~(1, 1, 1) PD
# (In + Dominated)
inferior1_s = np.array([[5, 1], [6, 2]]) # using 5 instead of 4 so there's
# enforceable Bully
inferior2_s = inferior1_s.transpose()
# no symmetric Cyclic games


# ~ (3, 3, 2) P1 has mixed strat maximin; P2 more strongly prefers maximin value
# to miscoordination outcome 
# (WW + Simple)
winwin1_a = np.array([[4, 1], [2, 3]])
winwin2_a = np.array([[4, 3], [-2, 2]])
# (1, 1, 4) Asym BOS
# (Bi + Temptation)
biased1_a = np.array([[3, 1], [4, 2]])
biased2_a = np.array([[1, 4], [3, 2]])
# (2, 4, 6) Evidently all options in this category have non-enforceable,
# at least for K=1
# (SB + Temptation (maybe Dom?))
second1_a = np.array([[4, 2], [1, 3]])
second2_a = np.array([[2, 4], [1, 3]])
# ~(4, 4, 1) 
# (Un + Temptation)
unfair1_a = np.array([[1, 4], [5, 2]])
unfair2_a = np.array([[5, 4], [2, 1]])
# ~(2, 2, 1) Asymmetric PD 
# (In + Dominated)
inferior1_a = np.array([[5, 1], [4, 2]])
inferior2_a = np.array([[4, 5], [1, 2]]) 
# ~(4, 4, 2)
# (Cy + Dominated)
cyclic1_a = np.array([[1, 4], [5, 2]])
cyclic2_a = np.array([[5, 4], [1, 2]])


#%%

# Games used for hyperparameter tuning
ch1 = np.array([[5, 3], [8, 0]])
ch2 = ch1.transpose()
pd1 = np.array([[3, 0], [4, 1]])
pd2 = pd1.transpose()
abos1 = np.array([[5, 0], [0, 1]])
abos2 = np.array([[1, 0], [0, 2]])
tr1 = np.array([[0, 1], [1/3, 2/3]])
tr2 = np.array([[1, 2/3], [0, 1/3]])

game_list = [(pd1, pd2), (ch1, ch2), (abos1, abos2), (tr1, tr2)]
rev_game_list = [(unfair1_s, unfair2_s), (unfair1_a, unfair2_a),
                   (winwin1_s, winwin2_s), (winwin1_a, winwin2_a),
                   (biased1_s, biased2_s), (biased1_a, biased2_a),
                   (second1_s, second2_s), (second1_a, second2_a),
                   (inferior1_s, inferior2_s), (inferior1_a, inferior2_a),
                   (cyclic1_a, cyclic2_a)
                  ]

game_names = ['uns', 'una', 'wws', 'wwa', 'bis', 'bia',
              'ses', 'sea', 'ins', 'ina', 'cya']
game_map = dict(zip(game_names, rev_game_list))

#%%

algo_order = []
algo_order = algo_order + [(0, i) for i in range(len(names_final_algo))]
algo_order = algo_order + [(j, 0) for j in range(1, len(names_final_algo))]
algo_order = algo_order + [(i, j) for i in range(1, len(names_final_algo)) for j in range(1, len(names_final_algo))]
combos = [(g, i, a[0], a[1]) for g in range(len(game_names)) for i in range(NUM_TRIALS) for a in algo_order]

#%%

if __name__ == '__main__':
    os.chdir('barg_results_pre')
    experiment_trial(int(sys.argv[1]))
